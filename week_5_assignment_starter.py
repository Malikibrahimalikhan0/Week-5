# -*- coding: utf-8 -*-
"""Week_5_assignment_starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15WG55Xfqs1NNQEbMwIJhtQZ6TWaSPyeW

### Name : Malik Ibrahim Ali Khan

# DS Automation Assignment

Using our prepared churn data from week 2:
- use pycaret to find an ML algorithm that performs best on the data
    - Choose a metric you think is best to use for finding the best model; by default, it is accuracy but it could be AUC, precision, recall, etc. The week 3 FTE has some information on these different metrics.
- save the model to disk
- create a Python script/file/module with a function that takes a pandas dataframe as an input and returns the probability of churn for each row in the dataframe
    - your Python file/function should print out the predictions for new data (new_churn_data.csv)
    - the true values for the new data are [1, 0, 0, 1, 0] if you're interested
- test your Python module and function with the new data, new_churn_data.csv
- write a short summary of the process and results at the end of this notebook
- upload this Jupyter Notebook and Python file to a Github repository, and turn in a link to the repository in the week 5 assignment dropbox

*Optional* challenges:
- return the probability of churn for each new prediction, and the percentile where that prediction is in the distribution of probability predictions from the training dataset (e.g. a high probability of churn like 0.78 might be at the 90th percentile)
- use other autoML packages, such as TPOT, H2O, MLBox, etc, and compare performance and features with pycaret
- create a class in your Python module to hold the functions that you created
- accept user input to specify a file using a tool such as Python's `input()` function, the `click` package for command-line arguments, or a GUI
- Use the unmodified churn data (new_unmodified_churn_data.csv) in your Python script. This will require adding the same preprocessing steps from week 2 since this data is like the original unmodified dataset from week 1.
"""

!pip install pycaret

import pandas as pd
import numpy as np
from pycaret.classification import *

# Week 2 Code for Data Preparation
df = pd.read_csv('churn_data.csv', index_col='customerID')
df.fillna(df['TotalCharges'].median(), inplace=True)
yn_dict = {'Yes': 1, 'No': 0}
df['PhoneService'] = df['PhoneService'].replace(yn_dict)
df['PaymentMethod'] = df['PaymentMethod'].replace({'Electronic check': 3, 'Mailed check': 2, 'Bank transfer (automatic)': 1, 'Credit card (automatic)': 0})
df['Contract'] = df['Contract'].replace({'Month-to-month': 0, 'One year': 1, 'Two year': 2})
df['Churn'] = df['Churn'].replace(yn_dict)
df.loc[df['tenure'] == 0, 'tenure'] = np.nan
df['tenure'].fillna(df['tenure'].median(), inplace=True)
df['charge_per_tenure'] = df['TotalCharges'] / df['tenure']

# Setup PyCaret
clf = setup(data=df, target='Churn', session_id=123, verbose=False)


# Compare models
best_model = compare_models(sort='AUC')

# Save the best model
save_model(best_model, 'best_model_churn')

new_data = pd.read_csv('new_churn_data.csv')

new_data.head()

import pandas as pd
from pycaret.classification import load_model, predict_model

def preprocess_data(df):
    yn_dict = {'Yes': 1, 'No': 0}
    df['PhoneService'] = df['PhoneService'].replace(yn_dict)
    df['PaymentMethod'] = df['PaymentMethod'].replace({'Electronic check': 3, 'Mailed check': 2, 'Bank transfer (automatic)': 1, 'Credit card (automatic)': 0})
    df['Contract'] = df['Contract'].replace({'Month-to-month': 0, 'One year': 1, 'Two year': 2})
    return df

def predict_churn(dataframe):
    model = load_model('best_model_churn')
    predictions = predict_model(model, data=dataframe)
    return predictions[['Label', 'Score']] if 'Label' in predictions.columns else predictions

if __name__ == "__main__":
    # Load new data
    new_data = pd.read_csv('new_churn_data.csv')
    # Preprocess the new data
    new_data_processed = preprocess_data(new_data)
    # Predict
    predictions = predict_churn(new_data_processed)
    print(predictions)

"""# Summary

Write a short summary of the process and results here.

### Process Summary
1. **Data Preprocessing**: Initially, the churn data was preprocessed. This included filling missing values, converting categorical variables to numerical representations, and creating new features.

2. **Model Selection with PyCaret**: Using PyCaret, multiple machine learning models were compared to find the best performer. The models were evaluated based on various metrics such as Accuracy, AUC, Recall, Precision, F1 Score, Kappa, and MCC.

3. **Best Model**: The Gradient Boosting Classifier (GBC) emerged as the best model with an Accuracy of 0.7929, AUC of 0.8376, and F1 Score of 0.5623.

4. **Model Saving**: The best model was saved for future use in making predictions.

5. **Prediction on New Data**: The saved model was then used to predict churn on a new dataset (`new_churn_data.csv`). The new data was preprocessed similarly to the training data.

### Results on New Data
The model made predictions on the new data, providing both the prediction labels (whether the customer would churn or not) and the prediction scores (the probability of churning). The results were as follows:

| CustomerID | Tenure | PhoneService | Contract | PaymentMethod | MonthlyCharges | TotalCharges | ChargePerTenure | PredictionLabel | PredictionScore |
|------------|--------|--------------|----------|---------------|----------------|--------------|-----------------|-----------------|-----------------|
| 9305-CKSKC | 22     | 1            | 0        | 2             | 97.40          | 811.70       | 36.90           | 0               | 0.5953          |
| 1452-KNGVK | 8      | 0            | 1        | 1             | 77.30          | 1701.95      | 212.74          | 0               | 0.9099          |
| 6723-OKKJM | 28     | 1            | 0        | 0             | 28.25          | 250.90       | 8.96            | 0               | 0.8978          |
| 7832-POPKP | 62     | 1            | 0        | 2             | 101.70         | 3106.56      | 50.11           | 1               | 0.5153          |
| 6348-TACGU | 10     | 0            | 2        | 1             | 51.15          | 3440.97      | 344.10          | 0               | 0.8580          |

### Interpretation
- The model predicted that 4 out of 5 customers in the new dataset are unlikely to churn, with relatively high prediction scores indicating confidence in these predictions.
- One customer (CustomerID: 7832-POPKP) was predicted to churn with a prediction score of 0.5153, suggesting a marginal likelihood of churn.

### Conclusions
- The Gradient Boosting Classifier proved to be the most effective model for this dataset based on the chosen evaluation metrics.
- The model's predictions on the new dataset provide valuable insights into customer behavior and can be instrumental in devising strategies to reduce churn.
"""